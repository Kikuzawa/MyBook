[[анализаторы]]
## Встроенные анализаторы

Elasticsearch предоставляет набор **встроенных анализаторов** (build-in analyzers).

Для их будем анализировать фразу `"- How old are you? - I'm 17."`.

Чтобы проверить, как работает конкретный анализатор, можно отправить следующий запрос.
```http
GET <ELASTICSEARCH_URL>/_analyze
Content-Type: application/json

{ 
  "analyzer": "analyzer_name",
  "text":"- How old are you? - I'm 17."
}
```

* **Стандартный**: `standard`. Используется по умолчанию. Разбивает текст на слова, переводит их в нижний регистр (`lowercase`), удаляет знаке препинания, при необходимости удаляет стоп-слова.
```js
/* terms */
["how", "old", "are", "you", "i'm", "17"]
```
* **Простой**: `simple`. Разделяет слова каждый раз, когда встречает не букву. Все термы переводятся в нижний регистр.
```js
/* terms */
["how", "old", "are", "you", "i", "m"]
```
* **Стоп-анализатор**: `stop`. Как `simple`, но с возможностью удалять стоп-слова. По умолчанию используются стоп-слова английского языка (вспомогательные глаголы, предлоги и так далее).
```js
/* terms */
["how", "old", "you", "i", "m"]
```
* **Пробельный**: `whitespace`. Разделяет текст, когда находит пробельные символы.
```js
/* terms */
["-", "How", "old", "are", "you?", "-", "I'm", "17."]
```
* **Анализатор ключевых слов**: `keyword`. Принимает текст и его возвращает как есть.
```js
/* terms */
["- How old are you? - I'm 17."]
```
* **Языковой**: `english`, `french`. Анализирует текст соответственно специфике языка. Удаляет стоп-слова, характерные языку. Переводит в нижний регистр.
```js
/* terms */
["how", "old", "you", "i'm", "17"]
```
* **Шаблонный**: `pattern`. Для разделения текста на термы использует регулярные выражения. По умолчанию используется регулярное выражение `\W+` (всё, что не может быть словом). Переводит в нижний регистр.
```js
/* terms */
["how", "old", "are", "you", "i", "m", "17"]
```

## Пользовательские анализаторы

Чтобы расширить функциональность встроенного анализатора (например, заменить стоп-слова или заменить регулярное выражение), необходимо создать **пользовательский анализатор** (custom analyzer) в настройках индекса (`settings`), что обычно делается при создании индекса.

Пользовательские анализаторы существуют в пределах индекса.

Например, создадим пользовательский анализатор, который игнорирует слово `old`. Для этого добавим его в поле `stopwords`.
```http
PUT <ELASTICSEARCH_URL>/index_name
Content-Type: application/json

{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_stop": {
          "type": "stop",
          "stopwords": ["old"]
        }
      }
    }
  }
}
```
Проверим, как анализируется текст `"- How old are you? - I'm 17."`.
```http
GET <ELASTICSEARCH_URL>/index_name/_analyze
Content-Type: application/json

{ 
  "analyzer": "analyzer_name",
  "text":"- How old are you? - I'm 17."
}
```
```js
/* terms */
["how", "are", "you", "i", "m"]
```

Для более гибкой настройки пользовательских анализаторов необходимо ознакомиться с блоками, из которых состоит каждый анализатор.

## Составляющие анализатора

*Анализатор* является *пакетом*, который *состоит* из *нескольких строительных блоков*: *фильтры символов*, *токенизатор*, *фильтры токенов*.

При *преобразовании текста* эти *блоки вызываются* в *указанном выше порядке*.

### Фильтр символов

**Фильтр символов** (Character filter) принимает оригинальный текст в качестве потока символов и трансформирует этот поток, добавляя, удаляя и изменяя символы.

Например, римские цифры (`I`, `II`, `III`) могут переводиться в арабские (1, 2, 3).

У анализатора может быть несколько фильтров символов или не быть вообще. Они применяются в указанном порядке.

### Фильтр токенов

**Фильтр токенов** (Token filter) принимает поток токенов (stream of tokens) и транмформирует его, удаляя, добавляя и изменяя токены.

Например, фильтр токенов `lowercase` переводит все токены в нижний регистр, фильтр `stop` удаляет стоп-слова, фильтр `synonym` добавляет синонимы в поток токенов.

У анализатора может быть несколько фильтров токенов или не быть вообще. Они применяются в указанном порядке.

<!-- Можно заменить, что английские стоп-слова перестали игнорироваться, поскольку произошла их перезапись массивом `["old"]`. Чтобы это исправить, можно использовать.
_english_ -->

## Граф токенов

Когда токенизатор превращает поток символов в поток токенов, он запоминает **позицию** (`position`) каждого токена в потоке и число позиций (`positionLength`), которые охватывает токен.

Это позволяет построить **ориентированный** (имеющий направление движения) **ациклический** (без циклов) **граф**, который называется **графом токенов** (token graph).

Каждая позиция представляет **вершину графа** (node).

Каждый токен представляет **дугу графа** (edge), указывающую на следующую позицию.

```js
/* граф токенов для текста "Hello our users!" после токенизации */

   hello      our       users    
0 ------> 1 -------> 2 -------> 3
```

Фильтры токенов могут добавлять новые токены (например, синонимы) в поток токенов, а значит и в граф токенов.

Синонимы записываются на ту же позицию, что и существующие токены.
```js
/* граф токенов для текста "Hello our users!" после токенизации
и фильтрации фильтром токенов с синонимами */

   hello      our       users    
0 ------> 1 -------> 2 -------> 3
    hi                 clients
```

### Многопозиционные токены

По умолчанию токен занимает только одну позицию, то есть его `positionLength` равняется 1.

Но некоторые синонимы могут занимать несколько позиций. Например, расшифровки аббревиатур (CSS, Cascading Style Sheets).
```js
  cascading      style       sheets         is          ...
0 ----------> 1 --------> 2 ---------> 3 ---------> 4 --------> 5
|                                      |
----------------------------------------
                  css
```

Фильтры, которые могут добавлять многопозиционные токен, называются **фильтрами токенов графа** ( graph token filters). Такими являются фильтры `synonym_graph` и `word_delimiter_graph`.

## Нормализация

В то время, как благодаря токенизации доступен полтотекстовый поиск, каждый отдельный токен при поиске сравнивается посимвольно.

* При поиске `How`, токен `how` не пройдёт проверку на совпадение.  
* При поиске `user`, токен `users` не пройдёт проверку на совпадение. 
* При поиске `hello`, токен `hi` не пройдёт проверку на совпадение.

Чтобы этого избежать, можно **нормализовать** (normalize) данные, то есть привести их к стандартному формату. Таким образом токены не будут точно совпадать (not exact match), но будут достаточно похожи, чтобы попасть в результат поиска.

К примеру, токен `Hello` может быть переведено в нижний регистру (`be lowercased`), `users` может быть приведён к его корневому слову `user` (stemmed), `hello` и `hi` являются синонимами и могут индексироваться как единственное слово `hello`.

## Анализатор индекса и анализатор поиска

*Анализ текста* осуществляется *дважды*
* при *индексации документа* (Index time)
* во *время поиска* (Search time, query time).

**Анализатор индекса** (Index analyzer) анализирует текстовые данные перед индексацией.

**Анализатор поиска** (Search analyzer) анализирует текс поискового запроса (`query`).

В большитсве случаев эти анализаторы должны иметь одинаковый набор правил токенизации и нормализации.

Например, при текст `"Hello our USERS!"` может быть преобразован анализатором индекса в `[hello, our, user]`, а текст поискового запроса `"Hi user"` — анализатором поиска в `[hello, user]`.

| Слово     | Поиск       | Индекс      |
| --------- | ----------- | ----------- |
| hello     |     +       |      +      |
| our       |             |      +      |
| user      |     +       |      +      |

Тогда документ со значением `"Hello our USERS!"` в текстовом поле попадёт в результат поиска по запросу `"Hi user"`.

### Разные анализаторы поиска и инекса 

Иногда может появиться необходимость использовать разные анализаторы индекса и поиска.

Например, когда мы хотим, чтобы убрать из поиска некоторые нежелательные результаты.

В этом случае можно указать отдельный анализатор для поиска.
```http
GET <ELASTICSEARCH_URL>/index_name/_search
Content-Type: application/json

{
  "query": {
    "match": {
      "message": {
        "query": "Hi user",
        "analyzer": "stop"
      }
    }
  }
}
```

Можно также задать разные анализаторы для конкретного поля при создании индекса в `mappings`.
```http
PUT <ELASTICSEARCH_URL>/index_name
Content-Type: application/json

{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "whitespace",
        "search_analyzer": "simple"
      }
    }
  }
}
```
