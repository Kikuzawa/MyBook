Батч-нормализация — метод ускорения глубокого обучения, предложенный Ioffe и Szegedy в начале 2015 года.

Внутренний ковариационный сдвиг - по мере распространения сигнала по сети пройдя через внутренние слои, он может сильно исказиться как по матожиднию, так и по дисперсии.

Нормализация входных данных таким образом, чтобы получить нулевое матожидание и единичную дисперсию

![[Pasted image 20241115104423.png]]