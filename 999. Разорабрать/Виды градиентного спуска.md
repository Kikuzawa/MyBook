# Пакетный градиентный спуск (BATCH GRADIENT DESCENT)

- Градиент функционала вычисляется как сумма градиентов, учитывая каждый элемент обучения сразу.
- Хорошо работает в случае выпуклых и относительно гладких функционалов, как например в задаче линейной или логистической регрессии.
- Поверхность, задаваемая функционалом ошибки нейронной сети, зачастую негладкая и имеет множество локальных экстремумов.
- Обилие обучающих данных, делает задачу поиска градиента по всем примерам затратной по памяти

# Метод схостического градиентного спуска

- Корректировка весов нейронной сети, используя аппроксимацию градиента функционала, вычисленную только на одном случайном обучающем примере из выборки.
- Метод привносит «шум» в процесс обучения, что позволяет (иногда) избежать локальных экстремумов.
- Шаги обучения происходят чаще, и не требуется держать в памяти градиенты всех обучающих примеров
![[Pasted image 20241114153241.png]]

# MINI-BATCH ГРАДИЕНТНЫЙ СПУСК

- Гибрид двух подходов SGD и BatchGD.
- Изменение параметров происходит, беря в расчет случайное подмножество примеров обучающей выборки (мини батч).
![[Pasted image 20241114153305.png]]

# Оптимизатор импульса

![[Pasted image 20241114153327.png]]

# Среднеквадратичное распространение

RMSprop — это экспоненциально затухающее среднее значение

![[Pasted image 20241114153402.png]]

# Оптимизатор ADAM

сочетает в себе идеи RMSProp и оптимизатора импульса.


алгоритм вычисляет экспоненциальное скользящее среднее градиента и квадратичный градиен

![[Pasted image 20241114153427.png]]

![[Pasted image 20241114153435.png]]

